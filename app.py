import pickle
import pandas as pd 
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from scipy import sparse 
from fastapi import FastAPI, Form
import pandas as pd
from starlette.responses import HTMLResponse
import uvicorn
# import mysql.connector
# from scipy.sparse import coo_matrix, vstack, hstack
from pydantic import BaseModel
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from sklearn.feature_extraction.text import TfidfTransformer
# from sklearn.cluster import KMeans
import pickle
from gensim.models import Word2Vec, Phrases
from pyvi import ViPosTagger, ViTokenizer #Tokenize tiáº¿ng Viá»‡t
import gensim
from gensim.models import Word2Vec
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from utils import *

dictionary = {
    "ğŸ‘¹": 'negative', "ğŸ‘»": 'positive', "ğŸ’ƒ": 'positive','ğŸ¤™': 'positive', 'ğŸ‘': 'positive',
    "ğŸ’„": 'positive', "ğŸ’": 'positive', "ğŸ’©": 'positive',"ğŸ˜•": 'negative', "ğŸ˜±": 'negative', "ğŸ˜¸": 'positive',
    "ğŸ˜¾": 'negative', "ğŸš«": 'negative',  "ğŸ¤¬": 'negative',"ğŸ§š": 'positive', "ğŸ§¡": 'positive','ğŸ¶': 'positive',
    'ğŸ‘': 'negative', 'ğŸ˜£': 'negative','âœ¨': 'positive', 'â£': 'positive','â˜€': 'positive',
    'â™¥': 'positive', 'ğŸ¤©': 'positive', 'like': 'positive', 'ğŸ’Œ': 'positive',
    'ğŸ¤£': 'positive', 'ğŸ–¤': 'positive', 'ğŸ¤¤': 'positive', 'ğŸ˜¢': 'negative',
    'â¤': 'positive', 'ğŸ˜': 'positive', 'ğŸ˜˜': 'positive', 'ğŸ˜ª': 'negative', 'ğŸ˜Š': 'positive',
    'ğŸ˜': 'positive', 'ğŸ’–': 'positive', 'ğŸ˜Ÿ': 'negative', 'ğŸ˜­': 'negative',
    'ğŸ’¯': 'positive', 'ğŸ’—': 'positive', 'â™¡': 'positive', 'ğŸ’œ': 'positive', 'ğŸ¤—': 'positive',
    'ğŸ˜¨': 'negative', 'â˜º': 'positive', 'ğŸ’‹': 'positive', 'ğŸ‘Œ': 'positive',
    'ğŸ˜–': 'negative', 'ğŸ˜€': 'positive', 'ğŸ˜¡': 'negative', 
    'ğŸ˜ ': 'negative', 'ğŸ˜’': 'negative', 'ğŸ™‚': 'positive', 'ğŸ˜': 'negative', 'ğŸ˜': 'positive', 
    'ğŸ˜™': 'positive', 'ğŸ˜¤': 'negative', 'ğŸ˜': 'positive', 'ğŸ˜†': 'positive', 'ğŸ’š': 'positive',
    'âœŒ': 'positive', 'ğŸ’•': 'positive', 'ğŸ˜': 'negative', 'ğŸ˜“': 'negative', 'ï¸ğŸ†—ï¸': 'positive',
    'ğŸ˜‰': 'positive', 'ğŸ˜‚': 'positive', ':v': 'positive', 'ğŸ˜‹': 'positive',
    'ğŸ’“': 'positive', 'ğŸ˜': 'negative', ':3': 'positive', 'ğŸ˜«': 'negative', 'ğŸ˜¥': 'negative',
    'ğŸ˜ƒ': 'positive', 'ğŸ˜¬': 'negative', 'ğŸ˜Œ': 'negative', 'ğŸ’›': 'positive', 'ğŸ¤': 'positive', 'ğŸˆ': 'positive',
    'ğŸ˜—': 'positive', 'ğŸ¤”': 'negative', 'ğŸ˜‘': 'negative', 'ğŸ”¥': 'negative', 'ğŸ™': 'negative',
    'ğŸ†—': 'positive', 'ğŸ˜»': 'positive', 'ğŸ’™': 'positive', 'ğŸ’Ÿ': 'positive',
    'ğŸ˜š': 'positive', 'âŒ': 'negative', 'ğŸ‘': 'positive',
    'ğŸŒ': 'positive',  'ğŸŒ·': 'positive', 'ğŸŒ¸': 'positive', 'ğŸŒº': ' positive ',
    'ğŸŒ¼': 'positive', 'ğŸ“': 'positive', 'ğŸ…': 'positive', 'ğŸ¾': 'positive', 'ğŸ‘‰': 'positive',
    'ğŸ’': 'positive', 'ğŸ’': 'positive', 'ğŸ’¥': 'positive', 'ğŸ’ª': 'positive', 'ğŸ‰': 'positive',
    'ğŸ’°': 'positive',  'ğŸ˜‡': 'positive', 'ğŸ˜›': 'positive', 'ğŸ˜œ': 'positive', 'ğŸ˜„': 'positive',
    'ğŸ™ƒ': 'positive', 'ğŸ¤‘': 'positive', 'ğŸ¤ª': 'positive','â˜¹': 'negative',  'ğŸ’€': 'negative',
    'ğŸ˜”': 'positive', 'ğŸ˜§': 'negative', 'ğŸ˜©': 'negative', 'ğŸ˜°': 'negative', 'ğŸ˜³': 'negative',
    'ğŸ˜µ': 'positive', 'ğŸ˜¶': 'negative', 'ğŸ™': 'negative',
    # 
    u'Ã´_kÃªi': 'ok', 'okie': 'ok', u'Ã´_kÃª': 'ok', 'okey': 'ok', u'Ã´kÃª': 'ok', 'oki': 'ok', 'oke': 'ok', 
    'okay': 'ok', 'tks': u'cÃ¡m_Æ¡n', 'thks': u'cÃ¡m_Æ¡n', 'thanks': u'cÃ¡m_Æ¡n', 'ths': u'cÃ¡m_Æ¡n', 'thank': u'cÃ¡m_Æ¡n', 
    u'cáº£m_Æ¡n': u'cÃ¡m_Æ¡n', 'â­': 'star', '*': 'star','ğŸŒŸ': 'star', 'kg': u'khÃ´ng', 'not': u'khÃ´ng', 
    'k': u'khÃ´ng', 'kh': u'khÃ´ng', u'kÃ´': u'khÃ´ng', 'hok': u'khÃ´ng', 'kp': u'khÃ´ng_pháº£i', 'ko': u'khÃ´ng', 
    'khong': u'khÃ´ng', 'he_he': 'positive','hehe': 'positive', 'hihi': 'positive', 'haha': 'positive', 
    'hjhj': 'positive', 'lol': 'negative','cc': 'negative', 'cute': u'dá»…_thÆ°Æ¡ng', 'huhu': 'negative', 'vs': u'vá»›i',
    'wa': u'quÃ¡', 'wÃ¡': u'quÃ¡', 'j': u'gÃ¬', 'sz': u'kÃ­ch_cá»¡', ' size': u'kÃ­ch_cá»¡', 'Ä‘x': u'Ä‘Æ°á»£c', 'dk': u'Ä‘Æ°á»£c',
    'dc': u'Ä‘Æ°á»£c', 'Ä‘k': u'Ä‘Æ°á»£c', 'Ä‘c': u'Ä‘Æ°á»£c','authentic': u'chÃ­nh_hÃ£ng','aut': u'chÃ­nh_hÃ£ng', 'auth': u'chÃ­nh_hÃ£ng', 
    'thick': u'dÃ y', 'store': u'cá»­a_hÃ ng', 'shop': u'cá»­a_hÃ ng', 'sp': u'sáº£n_pháº©m', 'gud': u'tá»‘t','weldone':'tá»‘t', 
    'good': u'tá»‘t', 'very': u'ráº¥t', u'gÃºt': u'tá»‘t', 'gut': u'tá»‘t', u'tot': u'tá»‘t', 'nice': 'tá»‘t', 'perfect': u'hoÃ n_háº£o', 
    'bt': u'bÃ¬nh_thÆ°á»ng', 'time': u'thá»i_gian', u'qÃ¡': u'quÃ¡', 'ship': u'giao_hÃ ng', 'm': u'mÃ¬nh', 'mik': u'mÃ¬nh', 
    'product': u'sáº£n_pháº©m', 'quality': u'cháº¥t_lÆ°á»£ng','chat': u'cháº¥t', 'excelent': u'hoÃ n_háº£o', 'bad': u'tá»‡',
    'fresh': u'tÆ°Æ¡i', 'sad': u'buá»“n', 'date': u'háº¡n_sá»­_dá»¥ng', 'hsd': u'háº¡n_sá»­_dá»¥ng','quickly': u'nhanh', 
    'quick': u'nhanh', 'fast': u'nhanh', 'delivery': u'giao_hÃ ng', u'sÃ­p': u'giao_hÃ ng', 'shipper': u'ngÆ°á»i_giao_hÃ ng', 
    'beautiful': u'Ä‘áº¹p', 'tl': u'tráº£_lá»i', 'r': u'rá»“i', 'shopE': u'cá»­a_hÃ ng', 'order': u'Ä‘áº·t_hÃ ng', 
    u'cháº¥t_lg': u'cháº¥t_lÆ°á»£ng', 'sd': u'sá»­_dá»¥ng', 'dt': u'Ä‘iá»‡n_thoáº¡i', 'nt': u'nháº¯n_tin', u'sÃ i': u'xÃ i', 
    'bjo': u'bao_giá»', 'thik': u'thÃ­ch', 'sop': u'cá»­a_hÃ ng', 'fb': 'facebook','dep': u'Ä‘áº¹p', 'xau': u'xáº¥u', 
    'delicious': u'ngon', u'hÃ g': u'hÃ ng', u'qá»§a': u'quáº£', 'iu': u'yÃªu', 'fake': u'giáº£_máº¡o', 'trl': u'tráº£_lá»i', 
    'por': u'tá»‡', 'poor': u'tá»‡', 'ib': u'nháº¯n_tin', 'rep': u'tráº£_lá»i', u'fback': 'feedback', 'fedback': 'feedback',
    'mn': u'má»i_ngÆ°á»i', 'cx': u'cÅ©ng', '&': u'vÃ '
}

stop_words = np.array([u'Ä‘Ã£', u'sáº½', u'khÃ´ng', u'nhÆ°ng', u'má»—i', u'chá»‰', u'vÃ¬', u'bá»Ÿi', u'bá»Ÿi_vÃ¬', u'cá»§a', u'khi', 
                       u'ai', u'thÃ¬', u'lÃ ', u'cÃ³_láº½', u'cÃ³_váº»', u'nhÆ°ng_mÃ ', u'tuy_nhiÃªn', u'vá»›i', u'vÃ ', u'bÃ¢y_giá»', 
                       u'cÃ³', u'vÃ i', u'Ã­t', u'láº§n_ná»¯a', u'má»™t_láº§n_ná»¯a', u'nÃ o', u'duy_nháº¥t', u'cho', 
                       u'Ä‘Ã¢y', u'Ä‘Ã³', u'kia', u'kÃ¬a', u'nÃ y', u'thÃ´i', u'cho_Ä‘áº¿n_khi', u'nÃªn', u'hoáº·c', u'mÃ ', 
                       u'cháº³ng', u'mÃ¬nh', u'tÃ´i', u'ná»¯a', u'táº¡i', u'kia', u'ráº±ng', u'bÃ¢y_giá»', u'Ä‘Æ°á»£c', u'bá»‹', 
                       u'sau_Ä‘Ã³', u'trong_lÃºc', u'trong_khi', u'cÃ¡i_gÃ¬', u'Ä‘áº¥y', u'Ä‘Ã³', u'vá»', u'cÃ²n', u'háº§u_háº¿t',
                       u'cÅ©ng', u'máº·c_dÃ¹', u'luÃ´n', u'má»™t', u'báº¥t_ká»³', u'báº¥t_kÃ¬', u'ná»¯a', u'á»Ÿ', u'trá»Ÿ_nÃªn', u'cÃ³_thá»ƒ',
                       u'cÃ³_láº½', u'hÆ¡n_ná»¯a', u'tháº­m_chÃ­', u'vÃ¬_váº­y', u'ráº¥t', u'quÃ¡', u'hÆ¡i_hÆ¡i', u'táº¡i_sao', u'láº¯m',
                       u'tháº¥y', u'nhiá»u', u'thá»±c_sá»±', u'lÃ¢u_lÃ¢u', u'Ä‘Ã¢u', u'Ä‘á»ƒ'])

class Sentence(BaseModel):
    sen: str

app = FastAPI()

origins = [
    "http://localhost",
    "http://localhost:8080"
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def transform(txt):
    txt = txt.lower()
    txt =  txt.replace(':3','')
    txt =  txt.replace(':v','')
    txt =  txt.replace(':o','')
    txt =  txt.replace('"','')
    txt = re.sub("[,#.:@;()$`~/-\^<>+=!]", '', txt)
    return txt

def tokenize(txt):
    return ViPosTagger.postagging(ViTokenizer.tokenize(txt))[0]

def transform_abbreviation(txt):
    rev_subs = {k:v for k,v in dictionary.items()}
    return [rev_subs.get(item,item)  for item in txt]

def remove_stopwords(input_text):
    whitelist = ["khÃ´ng", "khÃ´ng_thá»ƒ", "cháº³ng"]
    # words = input_text.split() 
    clean_words = [word for word in input_text if (word not in stop_words or word in whitelist) and len(word) > 1] 
    return " ".join(clean_words)

w2v_model = pickle.load(open("w2v_model",'rb'))
bow_vectorizer = pickle.load(open("bow_vectorizer",'rb'))

def word_vector(tokens, size):
    vec = np.zeros(size).reshape((1, size))
    count = 0
    for word in tokens:
        try:
            # print(w2v_model.wv[word].reshape((1, size)))
            vec += w2v_model.wv[word].reshape((1, size))
            count += 1.
        except KeyError:  # handling the case where the token is not in vocabulary
            print("ERROR while word to vector")
            continue
    if count != 0:
        vec /= count
    return vec

lor_th = 0.802978
rf_th = 0.731134
gb_th = 0.872183
pb_th = 0.988472

lor_model = pickle.load(open('logistic.sav','rb'))
rf_model = pickle.load(open('RF.sav','rb'))
gb_model = pickle.load(open('GB.sav','rb'))
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
model = AutoModelForSequenceClassification.from_pretrained("bdh240901/phoBert-BigData")

def pb_predict_func(text):
    text = segment_text((remove_any_special_left(remove_number_characters(remove_special_characters(text)))))
    model.to("cpu")
    input_values = tokenizer(text,return_tensors="pt",truncation=True).to("cpu")
    with torch.no_grad():
        logits = model(**input_values).logits
    # batch['label_id'] = torch.argmax(logits, dim=-1)
    # return batch
    return softmax(logits.numpy())[:,1]

# print(model.predict(word_vector(remove_stopwords(transform_abbreviation(tokenize(transform("cuá»™c hÃ nh_trÃ¬nh ngÃ n dáº·m pháº£i báº¯t_Ä‘áº§u báº±ng bÆ°á»›c_Ä‘i nhá»_bÃ© Ä‘áº§u_tiÃªn_cáº£m_Æ¡n bá»™ sÃ¡ch dá»‹ch happylive")))),300)))
# print(lor_model.predict_proba(bow_vectorizer.transform([remove_stopwords(transform_abbreviation(tokenize(transform("sáº£n pháº©m nÃ y ngon"))))]))[0,1] >= 0.880753)

# print(rf_model.predict_proba(bow_vectorizer.transform([remove_stopwords(transform_abbreviation(tokenize(transform("sáº£n pháº©m nÃ y ngon"))))]))[0,1] >= 0.880753)

# print(gb_model.predict_proba(bow_vectorizer.transform([remove_stopwords(transform_abbreviation(tokenize(transform("sáº£n pháº©m nÃ y ngon"))))]))[0,1] >= 0.933088)


# app.mount("/static", StaticFiles(directory="static"), name="static")

class Output(BaseModel):
    LOR: str
    RF: str
    GB: str
    PB: str

@app.post('/predict', response_model=Output)
async def predict(data : Sentence):
    gb_data = data.sen
    data = data.sen
    data = bow_vectorizer.transform([remove_stopwords(transform_abbreviation(tokenize(transform(data))))])
    lor_predict = "Positive" if lor_model.predict_proba(data)[0,1] >= lor_th else "Negative"
    rf_predict = "Positive" if rf_model.predict_proba(data)[0,1] >= rf_th else "Negative"
    gb_predict = "Positive" if gb_model.predict_proba(data)[0,1] >= gb_th else "Negative"
    pb_predict = "Positive" if pb_predict_func(gb_data)[0] >= gb_th else "Negative"
    return {'LOR': lor_predict, 'RF': rf_predict, 'GB': gb_predict, 'PB': pb_predict}

if __name__ == '__main__':
    uvicorn.run(app, host='127.0.0.1', port=8000, debug=True)
